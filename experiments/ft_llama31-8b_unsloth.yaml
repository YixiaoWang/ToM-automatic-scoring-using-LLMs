# From mistral-7b_m6kj82sv-22

model_id: "unsloth/Llama-3.1-8B-Instruct"

dynamic_padding: True
max_length: 8196
quantization_bits: 4

lora_r: 32
lora_alpha: 32 
lora_dropout: 0.34598713329540487

num_epochs: 8
batch_size: 2
gradient_accumulation_steps: 2

learning_rate: 0.00001165526147320343
weight_decay: 0.003597678135280804
lr_scheduler_type: "cosine"

early_stopping_patience: 25