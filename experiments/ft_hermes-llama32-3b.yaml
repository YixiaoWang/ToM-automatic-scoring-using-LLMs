model_id: "NousResearch/Hermes-3-Llama-3.2-3B"

dynamic_padding: True
max_length: 8196
quantization_bits: 8

lora_r: 32
lora_alpha: 32 
lora_dropout: 0.34598713329540487

num_epochs: 8
batch_size: 2
gradient_accumulation_steps: 2

learning_rate: 0.00001165526147320343
weight_decay: 0.003597678135280804
lr_scheduler_type: "cosine"

early_stopping_patience: 100
