program: hpsearch.py
entity: rshwndsz
project: odysseus
method: bayes
run_cap: 50
metric:
  goal: maximize
  name: eval_accuracy
parameters:
  model_id: 
    value: "meta-llama/Llama-3.2-3B-Instruct"
  num_labels:
    value: 2
  use_dynamic_padding:
    value: True
  max_length:
    value: 8196
  num_epochs:
    value: 1
  eval_steps:
    value: 128
  save_dir: 
    value: "./results"
  dataset_dir:
    value: "./data/dataset"
  data_fraction:
    value: 0.15

  learning_rate: 
    min: 1e-7
    max: 1e-3
    distribution: log_uniform_values
  weight_decay:
    min: 1e-3
    max: 1e-1
    distribution: log_uniform_values
  per_device_train_batch_size:
    values: [2, 4]
  gradient_accumulation_steps:
    values: [1, 2, 4]
  lr_scheduler_type: 
    values: ["linear", "cosine", "cosine_with_restarts", "reduce_lr_on_plateau"]
  quantization_bits: 
    values: [4, 8, None]
  lora_r: 
    values: [8, 16, 32, 64, 128]
  lora_alpha:
    values: [16, 32, 64, 128]
  lora_dropout:
    min: 1e-3
    max: 0.5
    distribution: log_uniform_values 

command:
  - ${env}
  - ${interpreter}
  - ${program}